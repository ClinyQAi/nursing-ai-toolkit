{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üè• Nursing FunctionGemma Training v2\n",
                "\n",
                "**IMPROVED VERSION** - This notebook trains the model to extract actual parameter values from clinical notes.\n",
                "\n",
                "**Key Improvements:**\n",
                "- 550+ diverse training examples\n",
                "- Higher LoRA rank (32) for better learning\n",
                "- 8 training epochs\n",
                "- Clearer system prompt with extraction cues"
            ],
            "metadata": {
                "id": "intro"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 1: Mount Google Drive"
            ],
            "metadata": {
                "id": "step1_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Create checkpoint directory\n",
                "output_dir = \"/content/drive/MyDrive/nursing-function-gemma-v2-checkpoints\"\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "print(f\"‚úÖ Checkpoints will be saved to: {output_dir}\")"
            ],
            "metadata": {
                "id": "step1_code"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 2: Install Dependencies"
            ],
            "metadata": {
                "id": "step2_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install -q -U torch bitsandbytes \n",
                "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
                "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
                "!pip install -q -U git+https://github.com/huggingface/trl.git\n",
                "!pip install -q -U accelerate datasets huggingface_hub"
            ],
            "metadata": {
                "id": "step2_code"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 3: Login to Hugging Face"
            ],
            "metadata": {
                "id": "step3_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from huggingface_hub import login\n",
                "login()"
            ],
            "metadata": {
                "id": "step3_code"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 4: Load Dataset (v2 - Diverse Examples)"
            ],
            "metadata": {
                "id": "step4_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Path to v2 dataset - MAKE SURE YOU UPLOADED THIS FILE!\n",
                "DATASET_PATH = \"/content/drive/MyDrive/nmc_brain/data/nursing_functions_dataset_v2.jsonl\"\n",
                "\n",
                "if not os.path.exists(DATASET_PATH):\n",
                "    print(f\"‚ùå ERROR: Dataset not found at {DATASET_PATH}\")\n",
                "    print(\"Please upload nursing_functions_dataset_v2.jsonl to your Drive!\")\n",
                "else:\n",
                "    dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
                "    print(f\"‚úÖ Loaded {len(dataset)} function examples (v2 diverse dataset)\")\n",
                "    print(f\"\\nSample example:\")\n",
                "    print(f\"  Input: {dataset[0]['instruction']}\")\n",
                "    print(f\"  Output: {dataset[0]['output']}\")"
            ],
            "metadata": {
                "id": "step4_code"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 5: Format Dataset for Training"
            ],
            "metadata": {
                "id": "step5_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "def formatting_prompts_func(example):\n",
                "    \"\"\"Format examples with clear extraction instructions.\"\"\"\n",
                "    output_texts = []\n",
                "    instructions = example['instruction']\n",
                "    outputs = example['output']\n",
                "    \n",
                "    if isinstance(instructions, str):\n",
                "        instructions = [instructions]\n",
                "        outputs = [outputs]\n",
                "        \n",
                "    for i in range(len(instructions)):\n",
                "        # Clear system prompt that teaches value extraction\n",
                "        tools_prompt = \"\"\"You are a clinical AI agent. Convert clinical notes into function calls.\n",
                "\n",
                "Functions:\n",
                "- record_vitals(systolic=X, diastolic=Y, heart_rate=Z, temp_c=T)\n",
                "- administer_medication(drug_name='X', dose='Y', route='Z')\n",
                "- search_nmc_standards(query='X')\n",
                "\n",
                "Extract the actual values from the input and output the correct function call.\"\"\"\n",
                "\n",
                "        inst = instructions[i]\n",
                "        out = outputs[i] if i < len(outputs) else \"error\"\n",
                "        \n",
                "        text = f\"<start_of_turn>user\\n{tools_prompt}\\n\\nInput: {inst}<end_of_turn>\\n<start_of_turn>model\\n{out}<end_of_turn>\"\n",
                "        output_texts.append(text)\n",
                "    return output_texts\n",
                "\n",
                "# Apply formatting\n",
                "dataset = dataset.map(lambda x: {\"text\": formatting_prompts_func(x)}, batched=True)\n",
                "print(f\"‚úÖ Dataset formatted! {len(dataset)} examples ready.\")"
            ],
            "metadata": {
                "id": "step5_code"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 6: Load Base Model (MedGemma 4B)"
            ],
            "metadata": {
                "id": "step6_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "import torch.distributed as dist\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import LoraConfig, prepare_model_for_kbit_training\n",
                "\n",
                "# Fix for Gemma 3 distributed requirement\n",
                "try:\n",
                "    if not dist.is_initialized():\n",
                "        dist.init_process_group(backend=\"gloo\", init_method=\"file:///tmp/dist_init_v2\", rank=0, world_size=1)\n",
                "    print(\"‚úÖ Distributed process group initialized\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è Warning: {e}\")\n",
                "\n",
                "# Model config\n",
                "MODEL_ID = \"google/medgemma-4b-it\"\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    bnb_4bit_use_double_quant=True\n",
                ")\n",
                "\n",
                "print(f\"Loading base model: {MODEL_ID}\")\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map={\"\": 0},\n",
                "    trust_remote_code=True\n",
                ")\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"right\"\n",
                "\n",
                "print(\"‚úÖ Model loaded!\")"
            ],
            "metadata": {
                "id": "step6_code"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 7: Configure LoRA (Increased Rank for Better Learning)"
            ],
            "metadata": {
                "id": "step7_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "peft_config = LoraConfig(\n",
                "    r=32,           # Increased from 16\n",
                "    lora_alpha=64,  # Increased from 32\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
                "    task_type=\"CAUSAL_LM\",\n",
                "    lora_dropout=0.05\n",
                ")\n",
                "\n",
                "print(\"‚úÖ LoRA config ready\")\n",
                "print(f\"   Rank: {peft_config.r}\")\n",
                "print(f\"   Alpha: {peft_config.lora_alpha}\")"
            ],
            "metadata": {
                "id": "step7_code"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 8: Setup Trainer"
            ],
            "metadata": {
                "id": "step8_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "# Custom data collator for Gemma 3\n",
                "def data_collator(features):\n",
                "    batch = tokenizer.pad(features, padding=True, return_tensors=\"pt\")\n",
                "    batch = dict(batch)\n",
                "    \n",
                "    if \"token_type_ids\" not in batch:\n",
                "        batch[\"token_type_ids\"] = torch.zeros_like(batch[\"input_ids\"])\n",
                "        \n",
                "    if \"labels\" not in batch:\n",
                "        labels = batch[\"input_ids\"].clone()\n",
                "        if tokenizer.pad_token_id is not None:\n",
                "            labels[labels == tokenizer.pad_token_id] = -100\n",
                "        batch[\"labels\"] = labels\n",
                "        \n",
                "    return batch\n",
                "\n",
                "# Training config - 8 epochs for better learning\n",
                "sft_config = SFTConfig(output_dir=output_dir)\n",
                "sft_config.max_seq_length = 512\n",
                "sft_config.dataset_text_field = \"text\"\n",
                "sft_config.num_train_epochs = 8         # Increased from 5\n",
                "sft_config.per_device_train_batch_size = 4\n",
                "sft_config.gradient_accumulation_steps = 4\n",
                "sft_config.learning_rate = 2e-4         # Slightly higher\n",
                "sft_config.fp16 = True\n",
                "sft_config.logging_steps = 10\n",
                "sft_config.save_steps = 100\n",
                "sft_config.push_to_hub = True\n",
                "sft_config.hub_model_id = \"NurseCitizenDeveloper/nursing-function-gemma\"\n",
                "sft_config.hub_private_repo = True\n",
                "sft_config.hub_strategy = \"checkpoint\"\n",
                "sft_config.packing = False\n",
                "\n",
                "# Create trainer\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=dataset,\n",
                "    peft_config=peft_config,\n",
                "    processing_class=tokenizer,\n",
                "    args=sft_config,\n",
                "    data_collator=data_collator\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Trainer ready!\")\n",
                "print(f\"   Dataset: {len(dataset)} examples\")\n",
                "print(f\"   Epochs: 8\")\n",
                "print(f\"   Batch size: 4 (effective: 16)\")"
            ],
            "metadata": {
                "id": "step8_code"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 9: Train! üöÄ"
            ],
            "metadata": {
                "id": "step9_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"üöÄ Starting training v2...\")\n",
                "print(\"   This will take approximately 2-3 hours on T4 GPU\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "trainer.train()\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"üéâ Training complete!\")"
            ],
            "metadata": {
                "id": "step9_code"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 10: Save and Push to Hub"
            ],
            "metadata": {
                "id": "step10_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Save locally\n",
                "trainer.model.save_pretrained(output_dir)\n",
                "tokenizer.save_pretrained(output_dir)\n",
                "print(f\"‚úÖ Model saved to: {output_dir}\")\n",
                "\n",
                "# Push to Hub\n",
                "trainer.push_to_hub()\n",
                "print(\"‚úÖ Pushed to Hugging Face Hub!\")\n",
                "print(\"\\nüéâ TRAINING v2 COMPLETE! ‚úÖ\")\n",
                "print(\"\\nYour improved model is now at:\")\n",
                "print(\"https://huggingface.co/NurseCitizenDeveloper/nursing-function-gemma\")"
            ],
            "metadata": {
                "id": "step10_code"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}