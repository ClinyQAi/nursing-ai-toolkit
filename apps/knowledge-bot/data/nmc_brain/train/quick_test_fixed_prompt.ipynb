{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸ”§ FunctionGemma Quick Test - Fixed Prompt\n\nThis tests the model WITHOUT the X,Y,Z placeholder issue."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "!pip install -q transformers peft torch accelerate bitsandbytes huggingface_hub"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "from huggingface_hub import login\n",
                "login()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "import torch.distributed as dist\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import PeftModel\n",
                "import re\n",
                "\n",
                "try:\n",
                "    if not dist.is_initialized():\n",
                "        dist.init_process_group(backend=\"gloo\", init_method=\"file:///tmp/fix_test\", rank=0, world_size=1)\n",
                "except: pass\n",
                "\n",
                "BASE_MODEL_ID = \"google/medgemma-4b-it\"\n",
                "ADAPTER_ID = \"NurseCitizenDeveloper/nursing-function-gemma\"\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
                "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, quantization_config=bnb_config, device_map={\"\": 0}, trust_remote_code=True)\n",
                "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "model = PeftModel.from_pretrained(model, ADAPTER_ID)\n",
                "print(\"âœ… Model loaded!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# FIXED PROMPT - No X,Y,Z placeholders!\n",
                "def test_with_fixed_prompt(user_input):\n",
                "    # Simple, clean prompt - just describe what to do\n",
                "    prompt = f\"\"\"<start_of_turn>user\n",
                "You are a clinical AI agent. Convert this clinical note into a function call.\n",
                "\n",
                "Available functions:\n",
                "- record_vitals: Records blood pressure, heart rate, temperature\n",
                "- administer_medication: Logs medication given\n",
                "- search_nmc_standards: Searches nursing guidelines\n",
                "\n",
                "Input: {user_input}\n",
                "\n",
                "Output the function call with the actual values from the input.<end_of_turn>\n",
                "<start_of_turn>model\n",
                "\"\"\"\n",
                "    \n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    if \"<start_of_turn>model\" in response:\n",
                "        response = response.split(\"<start_of_turn>model\")[-1].strip()\n",
                "    if \"<end_of_turn>\" in response:\n",
                "        response = response.split(\"<end_of_turn>\")[0].strip()\n",
                "    return response\n",
                "\n",
                "# Test cases\n",
                "tests = [\n",
                "    \"BP is 120/80, pulse 72\",\n",
                "    \"Gave Paracetamol 500mg orally\",\n",
                "    \"What does NMC say about confidentiality?\"\n",
                "]\n",
                "\n",
                "print(\"ðŸ§ª Testing with FIXED prompt (no placeholders):\")\n",
                "print(\"=\"*60)\n",
                "for t in tests:\n",
                "    result = test_with_fixed_prompt(t)\n",
                "    print(f\"\\nInput: {t}\")\n",
                "    print(f\"Output: {result}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Try even simpler - just ask directly\n",
                "def test_simple(user_input):\n",
                "    prompt = f\"\"\"<start_of_turn>user\n",
                "Convert to function call: {user_input}<end_of_turn>\n",
                "<start_of_turn>model\n",
                "\"\"\"\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    if \"<start_of_turn>model\" in response:\n",
                "        response = response.split(\"<start_of_turn>model\")[-1].strip()\n",
                "    return response\n",
                "\n",
                "print(\"\\n\\nðŸ§ª Testing with SIMPLE prompt:\")\n",
                "print(\"=\"*60)\n",
                "for t in tests:\n",
                "    result = test_simple(t)\n",
                "    print(f\"\\nInput: {t}\")\n",
                "    print(f\"Output: {result}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}