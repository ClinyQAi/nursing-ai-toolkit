{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üîç MedGemma Basic Generation Test\n\nDebug why the model generates nothing."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install -q transformers torch accelerate bitsandbytes huggingface_hub"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "from huggingface_hub import login\nlogin()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "import torch.distributed as dist\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "\n",
                "try:\n",
                "    if not dist.is_initialized():\n",
                "        dist.init_process_group(backend=\"gloo\", init_method=\"file:///tmp/basic_test\", rank=0, world_size=1)\n",
                "except: pass\n",
                "\n",
                "MODEL_ID = \"google/medgemma-4b-it\"\n",
                "print(f\"Loading {MODEL_ID}...\")\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_ID,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map={\"\": 0},\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
                "print(f\"‚úÖ Model loaded!\")\n",
                "print(f\"Pad token: {tokenizer.pad_token} ({tokenizer.pad_token_id})\")\n",
                "print(f\"EOS token: {tokenizer.eos_token} ({tokenizer.eos_token_id})\")\n",
                "print(f\"BOS token: {tokenizer.bos_token} ({tokenizer.bos_token_id})\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Test 1: Use the model's native chat template\n",
                "messages = [\n",
                "    {\"role\": \"user\", \"content\": \"Hello, can you help me? Say 'yes' if you can.\"}\n",
                "]\n",
                "\n",
                "# Apply native chat template\n",
                "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "print(\"PROMPT (native template):\")\n",
                "print(repr(prompt))\n",
                "print()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Generate\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "print(f\"Input tokens: {inputs['input_ids'].shape[1]}\")\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=30,\n",
                "        do_sample=False\n",
                "    )\n",
                "\n",
                "print(f\"Output tokens: {outputs.shape[1]}\")\n",
                "print(f\"New tokens: {outputs.shape[1] - inputs['input_ids'].shape[1]}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Decode full output\n",
                "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "print(\"\\nFULL OUTPUT:\")\n",
                "print(full_output)\n",
                "\n",
                "# Decode only new tokens\n",
                "new_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
                "new_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
                "print(\"\\nNEW TOKENS ONLY:\")\n",
                "print(f\"'{new_text}'\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Test 2: Simple medical question\n",
                "messages2 = [\n",
                "    {\"role\": \"user\", \"content\": \"What is a normal blood pressure reading?\"}\n",
                "]\n",
                "\n",
                "prompt2 = tokenizer.apply_chat_template(messages2, tokenize=False, add_generation_prompt=True)\n",
                "inputs2 = tokenizer(prompt2, return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs2 = model.generate(**inputs2, max_new_tokens=50, do_sample=False)\n",
                "\n",
                "new_tokens2 = outputs2[0][inputs2['input_ids'].shape[1]:]\n",
                "response2 = tokenizer.decode(new_tokens2, skip_special_tokens=True)\n",
                "print(\"\\nMedical question response:\")\n",
                "print(response2)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Test 3: Function calling prompt using native template\n",
                "fc_prompt = \"\"\"Convert this clinical note into a function call.\n",
                "\n",
                "Example:\n",
                "Input: BP 120/80, pulse 72\n",
                "Output: record_vitals(systolic=120, diastolic=80, heart_rate=72)\n",
                "\n",
                "Now convert:\n",
                "Input: BP is 110/70, pulse 68\n",
                "Output:\"\"\"\n",
                "\n",
                "messages3 = [{\"role\": \"user\", \"content\": fc_prompt}]\n",
                "prompt3 = tokenizer.apply_chat_template(messages3, tokenize=False, add_generation_prompt=True)\n",
                "inputs3 = tokenizer(prompt3, return_tensors=\"pt\").to(model.device)\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs3 = model.generate(**inputs3, max_new_tokens=50, do_sample=False)\n",
                "\n",
                "new_tokens3 = outputs3[0][inputs3['input_ids'].shape[1]:]\n",
                "response3 = tokenizer.decode(new_tokens3, skip_special_tokens=True)\n",
                "print(\"\\nFunction calling response:\")\n",
                "print(response3)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}