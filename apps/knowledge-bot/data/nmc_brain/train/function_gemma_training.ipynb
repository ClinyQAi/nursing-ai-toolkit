{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üè• Nursing FunctionGemma Training\n",
                "\n",
                "This notebook fine-tunes a model to perform **Nursing Function Calling** (Simulated EPR, RAG, etc.).\n",
                "It teaches the model to output structured `<function_call>` tags when appropriate."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mount Google Drive\n",
                "**Crucial Step:** We need to access your dataset and save the model!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Create checkpoint directory in Drive\n",
                "output_dir = \"/content/drive/My Drive/nursing-function-gemma-checkpoints\"\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "print(f\"Checkpoints will be saved to: {output_dir}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q -U torch bitsandbytes git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/trl.git accelerate datasets huggingface_hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.distributed as dist\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import LoraConfig, prepare_model_for_kbit_training\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "# --- üîß FIX: Initialize Distributed Process Group ---\n",
                "try:\n",
                "    if not dist.is_initialized():\n",
                "        dist.init_process_group(backend=\"gloo\", init_method=\"file:///tmp/unique_init_file\", rank=0, world_size=1)\n",
                "    print(\"‚úÖ Distributed process group initialized (dummy for single-GPU).\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è Warning: Could not init process group: {e}\")\n",
                "\n",
                "# 1. Config\n",
                "MODEL_ID = \"google/medgemma-4b-it\"\n",
                "DATASET_PATH = \"/content/drive/MyDrive/nmc_brain/data/nursing_functions_dataset.jsonl\"\n",
                "OUTPUT_DIR = output_dir\n",
                "\n",
                "# 2. Load Dataset\n",
                "if not os.path.exists(DATASET_PATH):\n",
                "    print(f\"ERROR: Dataset not found at {DATASET_PATH}. Please check your Drive path!\")\n",
                "else:\n",
                "    dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
                "    print(f\"Loaded {len(dataset)} function examples\")\n",
                "\n",
                "    # 3. Formatting Function\n",
                "    def formatting_prompts_func(example):\n",
                "        output_texts = []\n",
                "        instructions = example['instruction']\n",
                "        outputs = example['output']\n",
                "        \n",
                "        if isinstance(instructions, str):\n",
                "            instructions = [instructions]\n",
                "            outputs = [outputs]\n",
                "            \n",
                "        for i in range(len(instructions)):\n",
                "            tools_prompt = \"\"\"You are a clinical AI agent. You have access to the following tools:\n",
                "- record_vitals(systolic, diastolic, heart_rate, temp_c)\n",
                "- administer_medication(drug_name, dose, route)\n",
                "- search_nmc_standards(query)\n",
                "If the user's request requires a tool, output the function call XML.\"\"\"\n",
                "\n",
                "            inst = instructions[i]\n",
                "            out = outputs[i] if i < len(outputs) else \"error\"\n",
                "            \n",
                "            text = f\"<start_of_turn>user\\n{tools_prompt}\\n\\n{inst}<end_of_turn>\\n<start_of_turn>model\\n{out}<end_of_turn>\"\n",
                "            output_texts.append(text)\n",
                "        return output_texts\n",
                "\n",
                "    # 4. Pre-format\n",
                "    dataset = dataset.map(lambda x: {\"text\": formatting_prompts_func(x)}, batched=True)\n",
                "\n",
                "    # 5. Model Setup\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "        bnb_4bit_use_double_quant=True\n",
                "    )\n",
                "\n",
                "    device_map = {\"\": 0}\n",
                "\n",
                "    print(\"Loading model...\")\n",
                "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, quantization_config=bnb_config, device_map=device_map, trust_remote_code=True)\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    tokenizer.padding_side = \"right\"\n",
                "\n",
                "    # 6. LoRA\n",
                "    peft_config = LoraConfig(\n",
                "        r=16, lora_alpha=32, target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], task_type=\"CAUSAL_LM\"\n",
                "    )\n",
                "\n",
                "    # --- üîß FIX: Custom Data Collator for Gemma 3 (token_type_ids + labels) ---\n",
                "    def data_collator(features):\n",
                "        batch = tokenizer.pad(features, padding=True, return_tensors=\"pt\")\n",
                "        batch = dict(batch) # Make mutable\n",
                "        \n",
                "        # Gemma 3 REQUIRES token_type_ids\n",
                "        if \"token_type_ids\" not in batch:\n",
                "            batch[\"token_type_ids\"] = torch.zeros_like(batch[\"input_ids\"])\n",
                "            \n",
                "        # Generate labels (mask padding with -100)\n",
                "        if \"labels\" not in batch:\n",
                "            labels = batch[\"input_ids\"].clone()\n",
                "            if tokenizer.pad_token_id is not None:\n",
                "                labels[labels == tokenizer.pad_token_id] = -100\n",
                "            batch[\"labels\"] = labels\n",
                "            \n",
                "        return batch\n",
                "\n",
                "    # 7. SFTConfig\n",
                "    sft_config = SFTConfig(output_dir=OUTPUT_DIR)\n",
                "    sft_config.max_seq_length = 512\n",
                "    sft_config.dataset_text_field = \"text\"\n",
                "    sft_config.num_train_epochs = 5\n",
                "    sft_config.per_device_train_batch_size = 4\n",
                "    sft_config.gradient_accumulation_steps = 4\n",
                "    sft_config.fp16 = True\n",
                "    sft_config.logging_steps = 10\n",
                "    sft_config.push_to_hub = True\n",
                "    sft_config.hub_model_id = \"NurseCitizenDeveloper/nursing-function-gemma\"\n",
                "    sft_config.hub_private_repo = True\n",
                "    sft_config.hub_strategy = \"checkpoint\"\n",
                "    sft_config.packing = False\n",
                "\n",
                "    # 8. Trainer\n",
                "    trainer = SFTTrainer(\n",
                "        model=model,\n",
                "        train_dataset=dataset,\n",
                "        peft_config=peft_config,\n",
                "        processing_class=tokenizer,\n",
                "        args=sft_config,\n",
                "        data_collator=data_collator  # üîß ADDED to fix token_type_ids\n",
                "    )\n",
                "\n",
                "    trainer.train()\n",
                "    \n",
                "    # Save & Push final\n",
                "    trainer.model.save_pretrained(OUTPUT_DIR)\n",
                "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "    trainer.push_to_hub()\n",
                "    print(\"TRAINING COMPLETE! ‚úÖ\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}