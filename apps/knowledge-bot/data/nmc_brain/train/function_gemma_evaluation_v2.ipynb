{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# ðŸ“Š Nursing FunctionGemma Evaluation v2\n",
                "\n",
                "This notebook evaluates the **improved v2 model** trained on diverse examples.\n",
                "\n",
                "**Expected Improvement:**\n",
                "- Model should now extract actual VALUES (120, 80, 72)\n",
                "- Not just output function signatures"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 1: Mount Drive & Install"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "!pip install -q -U transformers peft torch accelerate bitsandbytes huggingface_hub pandas matplotlib seaborn tqdm"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 2: Login to Hugging Face"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from huggingface_hub import login\n",
                "login()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 3: Load Evaluation Dataset"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import json\n",
                "import os\n",
                "\n",
                "EVAL_PATH = \"/content/drive/MyDrive/nmc_brain/data/function_eval_dataset.json\"\n",
                "\n",
                "with open(EVAL_PATH, 'r') as f:\n",
                "    eval_data = json.load(f)\n",
                "\n",
                "print(f\"âœ… Loaded {len(eval_data)} evaluation examples\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 4: Load v2 Model"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "import torch.distributed as dist\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import PeftModel\n",
                "\n",
                "# Fix distributed\n",
                "try:\n",
                "    if not dist.is_initialized():\n",
                "        dist.init_process_group(backend=\"gloo\", init_method=\"file:///tmp/eval_init\", rank=0, world_size=1)\n",
                "except:\n",
                "    pass\n",
                "\n",
                "BASE_MODEL_ID = \"google/medgemma-4b-it\"\n",
                "ADAPTER_ID = \"NurseCitizenDeveloper/nursing-function-gemma\"\n",
                "\n",
                "print(\"Loading base model...\")\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    BASE_MODEL_ID,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map={\"\": 0},\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "print(f\"Loading adapter: {ADAPTER_ID}\")\n",
                "model = PeftModel.from_pretrained(model, ADAPTER_ID)\n",
                "print(\"âœ… v2 Model loaded!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 5: Define Prediction Function"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import re\n",
                "\n",
                "def generate_prediction(user_input):\n",
                "    \"\"\"Generate a function call prediction using v2 prompt format.\"\"\"\n",
                "    # Use same prompt format as training\n",
                "    tools_prompt = \"\"\"You are a clinical AI agent. Convert clinical notes into function calls.\n",
                "\n",
                "Functions:\n",
                "- record_vitals(systolic=X, diastolic=Y, heart_rate=Z, temp_c=T)\n",
                "- administer_medication(drug_name='X', dose='Y', route='Z')\n",
                "- search_nmc_standards(query='X')\n",
                "\n",
                "Extract the actual values from the input and output the correct function call.\"\"\"\n",
                "\n",
                "    prompt = f\"<start_of_turn>user\\n{tools_prompt}\\n\\nInput: {user_input}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
                "    \n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=100,\n",
                "            do_sample=False,\n",
                "            eos_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    if \"<start_of_turn>model\" in response:\n",
                "        response = response.split(\"<start_of_turn>model\")[-1].strip()\n",
                "    if \"<end_of_turn>\" in response:\n",
                "        response = response.split(\"<end_of_turn>\")[0].strip()\n",
                "    \n",
                "    # Extract function call\n",
                "    match = re.search(r'<function_call>(.*?)</function_call>', response, re.DOTALL)\n",
                "    if match:\n",
                "        return match.group(1).strip()\n",
                "    \n",
                "    # Try raw function call\n",
                "    for func_name in ['record_vitals', 'administer_medication', 'search_nmc_standards']:\n",
                "        if func_name in response:\n",
                "            match = re.search(rf'{func_name}\\([^)]+\\)', response)\n",
                "            if match:\n",
                "                return match.group(0)\n",
                "    \n",
                "    return response\n",
                "\n",
                "# Quick test\n",
                "test_result = generate_prediction(\"BP is 120/80, pulse 72\")\n",
                "print(f\"Quick test result: {test_result}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 6: Run Full Evaluation"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "from tqdm import tqdm\n",
                "\n",
                "print(\"Running evaluation on 50 test cases...\")\n",
                "results = []\n",
                "\n",
                "for item in tqdm(eval_data):\n",
                "    user_input = item['input']\n",
                "    expected = item['expected']\n",
                "    predicted = generate_prediction(user_input)\n",
                "    \n",
                "    # Determine function type\n",
                "    if 'record_vitals' in expected:\n",
                "        func_type = 'vitals'\n",
                "    elif 'administer_medication' in expected:\n",
                "        func_type = 'medication'\n",
                "    else:\n",
                "        func_type = 'nmc_search'\n",
                "    \n",
                "    # Function name match\n",
                "    func_name_expected = expected.split('(')[0]\n",
                "    func_name_correct = func_name_expected in predicted\n",
                "    \n",
                "    # Exact match (normalized - remove spaces)\n",
                "    expected_norm = re.sub(r'\\s+', '', expected.lower())\n",
                "    predicted_norm = re.sub(r'\\s+', '', predicted.lower())\n",
                "    exact_match = expected_norm == predicted_norm\n",
                "    \n",
                "    # Value extraction check (has actual numbers, not just param names)\n",
                "    has_values = bool(re.search(r'=\\d+', predicted) or re.search(r\"='[^']+'\", predicted))\n",
                "    \n",
                "    results.append({\n",
                "        'input': user_input,\n",
                "        'expected': expected,\n",
                "        'predicted': predicted,\n",
                "        'func_type': func_type,\n",
                "        'func_name_correct': func_name_correct,\n",
                "        'exact_match': exact_match,\n",
                "        'has_values': has_values\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "print(f\"\\nâœ… Evaluation complete! {len(df)} examples processed.\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 7: Results Summary"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "overall_accuracy = df['exact_match'].mean() * 100\n",
                "func_name_accuracy = df['func_name_correct'].mean() * 100\n",
                "value_extraction = df['has_values'].mean() * 100\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"ðŸ“Š v2 MODEL RESULTS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"\\nðŸŽ¯ Exact Match Accuracy: {overall_accuracy:.1f}%\")\n",
                "print(f\"ðŸ”§ Function Name Accuracy: {func_name_accuracy:.1f}%\")\n",
                "print(f\"ðŸ“ Value Extraction Rate: {value_extraction:.1f}%\")\n",
                "print(f\"ðŸ“Š Total Test Cases: {len(df)}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"ðŸ“Š ACCURACY BY FUNCTION TYPE\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for func_type in ['vitals', 'medication', 'nmc_search']:\n",
                "    subset = df[df['func_type'] == func_type]\n",
                "    if len(subset) > 0:\n",
                "        acc = subset['exact_match'].mean() * 100\n",
                "        func_acc = subset['func_name_correct'].mean() * 100\n",
                "        val_rate = subset['has_values'].mean() * 100\n",
                "        print(f\"\\n{func_type.upper()}:\")\n",
                "        print(f\"  - Count: {len(subset)}\")\n",
                "        print(f\"  - Exact Match: {acc:.1f}%\")\n",
                "        print(f\"  - Function Name Match: {func_acc:.1f}%\")\n",
                "        print(f\"  - Value Extraction: {val_rate:.1f}%\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 8: Sample Predictions"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"ðŸ“‹ SAMPLE PREDICTIONS\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for i, row in df.head(10).iterrows():\n",
                "    status = \"âœ…\" if row['exact_match'] else \"âŒ\"\n",
                "    print(f\"\\n{status} Example {i+1}:\")\n",
                "    print(f\"   Input: {row['input'][:60]}...\")\n",
                "    print(f\"   Expected: {row['expected']}\")\n",
                "    print(f\"   Predicted: {row['predicted']}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## Step 9: Summary for Article"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ðŸ“‹ SUMMARY STATISTICS FOR ARTICLE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\"\"\n",
                "The Nursing FunctionGemma v2 model was evaluated on a held-out test set\n",
                "of {len(df)} clinical scenarios across three function types.\n",
                "\n",
                "OVERALL PERFORMANCE:\n",
                "- Exact Match Accuracy: {overall_accuracy:.1f}%\n",
                "- Function Name Accuracy: {func_name_accuracy:.1f}%\n",
                "- Value Extraction Rate: {value_extraction:.1f}%\n",
                "\n",
                "PERFORMANCE BY FUNCTION TYPE:\"\"\")\n",
                "\n",
                "for func_type in ['vitals', 'medication', 'nmc_search']:\n",
                "    subset = df[df['func_type'] == func_type]\n",
                "    if len(subset) > 0:\n",
                "        acc = subset['exact_match'].mean() * 100\n",
                "        print(f\"- {func_type.title()}: {acc:.1f}% (n={len(subset)})\")\n",
                "\n",
                "print(f\"\"\"\n",
                "TRAINING DETAILS (v2):\n",
                "- Base Model: google/medgemma-4b-it\n",
                "- Fine-tuning Method: QLoRA (4-bit quantization)\n",
                "- Training Examples: 550\n",
                "- Epochs: 8\n",
                "- LoRA Rank: 32\n",
                "- Final Training Loss: 0.077\n",
                "\"\"\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}