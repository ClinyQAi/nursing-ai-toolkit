{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ“Š Nursing FunctionGemma Evaluation\n",
                "\n",
                "This notebook evaluates the **Nursing FunctionGemma** model's function-calling accuracy.\n",
                "\n",
                "**Metrics Calculated:**\n",
                "- Overall Accuracy\n",
                "- Function Type Accuracy (vitals, medication, NMC search)\n",
                "- Parameter Extraction Accuracy\n",
                "- Error Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q -U transformers peft torch accelerate bitsandbytes huggingface_hub pandas matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "login()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import json\n",
                "import re\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import PeftModel\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Load evaluation dataset\n",
                "EVAL_PATH = \"/content/drive/MyDrive/nmc_brain/data/function_eval_dataset.json\"\n",
                "\n",
                "with open(EVAL_PATH, 'r') as f:\n",
                "    eval_data = json.load(f)\n",
                "\n",
                "print(f\"Loaded {len(eval_data)} evaluation examples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "BASE_MODEL_ID = \"google/medgemma-4b-it\"\n",
                "ADAPTER_ID = \"NurseCitizenDeveloper/nursing-function-gemma\"\n",
                "\n",
                "print(\"Loading base model...\")\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    BASE_MODEL_ID,\n",
                "    quantization_config=bnb_config,\n",
                "    device_map=\"auto\",\n",
                "    trust_remote_code=True\n",
                ")\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "print(f\"Loading adapter from: {ADAPTER_ID}\")\n",
                "model = PeftModel.from_pretrained(model, ADAPTER_ID)\n",
                "print(\"âœ… Model loaded!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_prediction(user_input):\n",
                "    \"\"\"Generate a function call prediction.\"\"\"\n",
                "    tools_prompt = \"\"\"You are a clinical AI agent. You have access to the following tools:\n",
                "- record_vitals(systolic, diastolic, heart_rate, temp_c)\n",
                "- administer_medication(drug_name, dose, route)\n",
                "- search_nmc_standards(query)\n",
                "If the user's request requires a tool, output the function call XML.\"\"\"\n",
                "\n",
                "    prompt = f\"<start_of_turn>user\\n{tools_prompt}\\n\\n{user_input}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
                "    \n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=100,\n",
                "            do_sample=False,\n",
                "            temperature=0.1,\n",
                "            eos_token_id=tokenizer.eos_token_id\n",
                "        )\n",
                "    \n",
                "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    \n",
                "    if \"<start_of_turn>model\" in response:\n",
                "        response = response.split(\"<start_of_turn>model\")[-1].strip()\n",
                "    if \"<end_of_turn>\" in response:\n",
                "        response = response.split(\"<end_of_turn>\")[0].strip()\n",
                "    \n",
                "    # Extract function call\n",
                "    match = re.search(r'<function_call>(.*?)</function_call>', response, re.DOTALL)\n",
                "    if match:\n",
                "        return match.group(1).strip()\n",
                "    \n",
                "    # Try to find raw function call\n",
                "    for func_name in ['record_vitals', 'administer_medication', 'search_nmc_standards']:\n",
                "        if func_name in response:\n",
                "            match = re.search(rf'{func_name}\\(.*?\\)', response)\n",
                "            if match:\n",
                "                return match.group(0)\n",
                "    \n",
                "    return response"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Run evaluation\n",
                "print(\"Running evaluation...\")\n",
                "results = []\n",
                "\n",
                "for item in tqdm(eval_data):\n",
                "    user_input = item['input']\n",
                "    expected = item['expected']\n",
                "    predicted = generate_prediction(user_input)\n",
                "    \n",
                "    # Determine function type\n",
                "    if 'record_vitals' in expected:\n",
                "        func_type = 'vitals'\n",
                "    elif 'administer_medication' in expected:\n",
                "        func_type = 'medication'\n",
                "    else:\n",
                "        func_type = 'nmc_search'\n",
                "    \n",
                "    # Check correctness\n",
                "    # Function name match\n",
                "    func_name_expected = expected.split('(')[0]\n",
                "    func_name_correct = func_name_expected in predicted\n",
                "    \n",
                "    # Exact match (normalized)\n",
                "    expected_norm = re.sub(r'\\s+', '', expected.lower())\n",
                "    predicted_norm = re.sub(r'\\s+', '', predicted.lower())\n",
                "    exact_match = expected_norm == predicted_norm\n",
                "    \n",
                "    results.append({\n",
                "        'input': user_input,\n",
                "        'expected': expected,\n",
                "        'predicted': predicted,\n",
                "        'func_type': func_type,\n",
                "        'func_name_correct': func_name_correct,\n",
                "        'exact_match': exact_match\n",
                "    })\n",
                "\n",
                "df = pd.DataFrame(results)\n",
                "print(f\"\\nâœ… Evaluation complete! {len(df)} examples processed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“ˆ Results Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate overall metrics\n",
                "overall_accuracy = df['exact_match'].mean() * 100\n",
                "func_name_accuracy = df['func_name_correct'].mean() * 100\n",
                "\n",
                "print(\"=\"*50)\n",
                "print(\"ðŸ“Š OVERALL RESULTS\")\n",
                "print(\"=\"*50)\n",
                "print(f\"\\nðŸŽ¯ Exact Match Accuracy: {overall_accuracy:.1f}%\")\n",
                "print(f\"ðŸ”§ Function Name Accuracy: {func_name_accuracy:.1f}%\")\n",
                "print(f\"ðŸ“ Total Test Cases: {len(df)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Accuracy by function type\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"ðŸ“Š ACCURACY BY FUNCTION TYPE\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for func_type in ['vitals', 'medication', 'nmc_search']:\n",
                "    subset = df[df['func_type'] == func_type]\n",
                "    if len(subset) > 0:\n",
                "        acc = subset['exact_match'].mean() * 100\n",
                "        func_acc = subset['func_name_correct'].mean() * 100\n",
                "        print(f\"\\n{func_type.upper()}:\")\n",
                "        print(f\"  - Count: {len(subset)}\")\n",
                "        print(f\"  - Exact Match: {acc:.1f}%\")\n",
                "        print(f\"  - Function Name Match: {func_acc:.1f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Pie chart - overall accuracy\n",
                "correct = df['exact_match'].sum()\n",
                "incorrect = len(df) - correct\n",
                "axes[0].pie([correct, incorrect], labels=['Correct', 'Incorrect'], \n",
                "            autopct='%1.1f%%', colors=['#2ecc71', '#e74c3c'])\n",
                "axes[0].set_title('Overall Exact Match Accuracy')\n",
                "\n",
                "# Bar chart - accuracy by function type\n",
                "func_types = ['vitals', 'medication', 'nmc_search']\n",
                "accuracies = [df[df['func_type']==ft]['exact_match'].mean()*100 for ft in func_types]\n",
                "bars = axes[1].bar(func_types, accuracies, color=['#3498db', '#9b59b6', '#e67e22'])\n",
                "axes[1].set_ylabel('Accuracy (%)')\n",
                "axes[1].set_title('Accuracy by Function Type')\n",
                "axes[1].set_ylim(0, 100)\n",
                "for bar, acc in zip(bars, accuracies):\n",
                "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
                "                 f'{acc:.1f}%', ha='center')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('/content/drive/MyDrive/nmc_brain/evaluation_results.png', dpi=150)\n",
                "plt.show()\n",
                "print(\"\\nðŸ“¸ Chart saved to: /content/drive/MyDrive/nmc_brain/evaluation_results.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ” Error Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show errors\n",
                "errors = df[df['exact_match'] == False]\n",
                "\n",
                "print(f\"\\nâŒ Total Errors: {len(errors)} / {len(df)}\")\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"ERROR EXAMPLES\")\n",
                "print(\"=\"*50)\n",
                "\n",
                "for i, row in errors.head(10).iterrows():\n",
                "    print(f\"\\n--- Example {i+1} ---\")\n",
                "    print(f\"Input: {row['input']}\")\n",
                "    print(f\"Expected: {row['expected']}\")\n",
                "    print(f\"Predicted: {row['predicted']}\")\n",
                "    print(f\"Function Name Correct: {row['func_name_correct']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results to CSV\n",
                "df.to_csv('/content/drive/MyDrive/nmc_brain/evaluation_results.csv', index=False)\n",
                "print(\"\\nðŸ’¾ Results saved to: /content/drive/MyDrive/nmc_brain/evaluation_results.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ“‹ Summary Statistics for Article"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate summary for article\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ðŸ“‹ SUMMARY STATISTICS FOR ARTICLE\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\"\"\n",
                "The Nursing FunctionGemma model was evaluated on a held-out test set\n",
                "of {len(df)} clinical scenarios across three function types:\n",
                "\n",
                "OVERALL PERFORMANCE:\n",
                "- Exact Match Accuracy: {overall_accuracy:.1f}%\n",
                "- Function Name Accuracy: {func_name_accuracy:.1f}%\n",
                "\n",
                "PERFORMANCE BY FUNCTION TYPE:\n",
                "\"\"\")\n",
                "\n",
                "for func_type in ['vitals', 'medication', 'nmc_search']:\n",
                "    subset = df[df['func_type'] == func_type]\n",
                "    if len(subset) > 0:\n",
                "        acc = subset['exact_match'].mean() * 100\n",
                "        print(f\"- {func_type.title()}: {acc:.1f}% (n={len(subset)})\")\n",
                "\n",
                "print(f\"\"\"\n",
                "TRAINING DETAILS:\n",
                "- Base Model: google/medgemma-4b-it\n",
                "- Fine-tuning Method: QLoRA (4-bit quantization)\n",
                "- Training Examples: 500\n",
                "- Epochs: 5\n",
                "- Final Training Loss: 0.74\n",
                "\n",
                "The model demonstrates strong capability in converting natural language\n",
                "clinical statements into structured function calls, with particularly\n",
                "high accuracy in {df.groupby('func_type')['exact_match'].mean().idxmax()} tasks.\n",
                "\"\"\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}