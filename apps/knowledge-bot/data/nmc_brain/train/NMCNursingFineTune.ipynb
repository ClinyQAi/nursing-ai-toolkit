{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Nursing Proficiency+ Fine-Tuning (Memory Optimized)\n",
                "\n",
                "Optimized for Colab T4 GPU with 15GB VRAM."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 1: Mount Drive & Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "drive.mount('/content/drive')\n",
                "project_folder = \"/content/drive/My Drive/nmc_brain\"\n",
                "os.makedirs(project_folder, exist_ok=True)\n",
                "print(f\"✅ Project: {project_folder}\")\n",
                "%cd \"$project_folder\"\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 2: Install & Create Memory-Efficient Script"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!mkdir -p train data\n",
                "with open('requirements.txt', 'w') as f:\n",
                "    f.write(\"transformers>=4.40.0\\npeft>=0.10.0\\nbitsandbytes>=0.43.0\\ndatasets>=2.18.0\\naccelerate>=0.29.0\\ntrl>=0.12.0\\nhuggingface_hub\\npython-dotenv\\nsentencepiece\")\n",
                "!pip install -q -r requirements.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open('train/fine_tune_medgemma.py', 'w') as f:\n",
                "    f.write('''import os, torch, argparse\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
                "from trl import SFTConfig, SFTTrainer\n",
                "from huggingface_hub import login\n",
                "\n",
                "model_id = \"google/medgemma-4b-it\"\n",
                "hub_id = \"NurseCitizenDeveloper/nursing-proficiency-plus\"\n",
                "\n",
                "def parse_args():\n",
                "    p = argparse.ArgumentParser()\n",
                "    p.add_argument(\"--output_dir\", type=str, default=\"./checkpoints\")\n",
                "    return p.parse_args()\n",
                "\n",
                "def fine_tune(args, hf_token=None):\n",
                "    if hf_token: login(token=hf_token)\n",
                "    dataset = load_dataset(\"json\", data_files=\"data/nmc_dataset.jsonl\", split=\"train\")\n",
                "    bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)\n",
                "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb, device_map=\"auto\", trust_remote_code=True)\n",
                "    model.config.use_cache = False\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "    model.gradient_checkpointing_enable()\n",
                "    tok = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
                "    tok.pad_token = tok.eos_token\n",
                "    tok.padding_side = \"right\"\n",
                "    peft = LoraConfig(lora_alpha=16, lora_dropout=0.1, r=64, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"])\n",
                "    model = get_peft_model(model, peft)\n",
                "    \n",
                "    cfg = SFTConfig(output_dir=args.output_dir)\n",
                "    cfg.dataset_text_field = \"output\"\n",
                "    cfg.max_seq_length = 512\n",
                "    cfg.num_train_epochs = 3\n",
                "    cfg.per_device_train_batch_size = 1\n",
                "    cfg.gradient_accumulation_steps = 16\n",
                "    cfg.optim = \"paged_adamw_8bit\"\n",
                "    cfg.save_strategy = \"steps\"\n",
                "    cfg.save_steps = 50\n",
                "    cfg.save_total_limit = 2\n",
                "    cfg.logging_steps = 5\n",
                "    cfg.learning_rate = 2e-4\n",
                "    cfg.weight_decay = 0.001\n",
                "    cfg.bf16 = True\n",
                "    cfg.gradient_checkpointing = True\n",
                "    cfg.push_to_hub = True\n",
                "    cfg.hub_model_id = hub_id\n",
                "    cfg.hub_private_repo = True\n",
                "    cfg.hub_strategy = \"checkpoint\"\n",
                "    cfg.report_to = \"none\"\n",
                "    \n",
                "    trainer = SFTTrainer(model=model, train_dataset=dataset, peft_config=peft, processing_class=tok, args=cfg)\n",
                "    ckpt = True if os.path.isdir(args.output_dir) and any(d.startswith(\"checkpoint\") for d in os.listdir(args.output_dir)) else None\n",
                "    trainer.train(resume_from_checkpoint=ckpt)\n",
                "    trainer.model.save_pretrained(args.output_dir)\n",
                "    tok.save_pretrained(args.output_dir)\n",
                "    trainer.push_to_hub()\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    fine_tune(parse_args(), hf_token=os.getenv(\"HF_TOKEN\"))''')\n",
                "print(\"✅ Memory-optimized script ready\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 3: HF Login"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ[\"HF_TOKEN\"] = \"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\"\n",
                "from huggingface_hub import login\n",
                "login(token=os.environ[\"HF_TOKEN\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Step 4: Train (Restart from Checkpoint)\n",
                "The script will automatically resume from step 45!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if not os.path.exists('data/nmc_dataset.jsonl'):\n",
                "    print(\"❌ Upload nmc_dataset.jsonl to:\", os.getcwd() + \"/data/\")\n",
                "else:\n",
                "    !mkdir -p checkpoints_nursing_plus\n",
                "    !python train/fine_tune_medgemma.py --output_dir checkpoints_nursing_plus"
            ]
        }
    ],
    "metadata": {
        "kernelnel": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}