{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üîç FunctionGemma Debug Test\n\nShows raw model output to diagnose the issue."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "!pip install -q transformers peft torch accelerate bitsandbytes huggingface_hub"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "from huggingface_hub import login\n",
                "login()"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "import torch\n",
                "import torch.distributed as dist\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
                "from peft import PeftModel\n",
                "\n",
                "try:\n",
                "    if not dist.is_initialized():\n",
                "        dist.init_process_group(backend=\"gloo\", init_method=\"file:///tmp/debug_test\", rank=0, world_size=1)\n",
                "except: pass\n",
                "\n",
                "BASE_MODEL_ID = \"google/medgemma-4b-it\"\n",
                "ADAPTER_ID = \"NurseCitizenDeveloper/nursing-function-gemma\"\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
                "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_ID, quantization_config=bnb_config, device_map={\"\": 0}, trust_remote_code=True)\n",
                "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, trust_remote_code=True)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "model = PeftModel.from_pretrained(model, ADAPTER_ID)\n",
                "print(\"‚úÖ Model loaded!\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# DEBUG TEST - Use EXACT format from training\n",
                "user_input = \"BP is 120/80, pulse 72\"\n",
                "\n",
                "# This EXACT format was used in training:\n",
                "tools_prompt = \"\"\"You are a clinical AI agent. Convert clinical notes into function calls.\n",
                "\n",
                "Functions:\n",
                "- record_vitals(systolic=X, diastolic=Y, heart_rate=Z, temp_c=T)\n",
                "- administer_medication(drug_name='X', dose='Y', route='Z')\n",
                "- search_nmc_standards(query='X')\n",
                "\n",
                "Extract the actual values from the input and output the correct function call.\"\"\"\n",
                "\n",
                "prompt = f\"<start_of_turn>user\\n{tools_prompt}\\n\\nInput: {user_input}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"PROMPT BEING SENT:\")\n",
                "print(\"=\"*60)\n",
                "print(prompt)\n",
                "print(\"=\"*60)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Generate with explicit settings\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "print(f\"Input tokens: {inputs['input_ids'].shape}\")\n",
                "\n",
                "with torch.no_grad():\n",
                "    outputs = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=50,\n",
                "        do_sample=False,\n",
                "        eos_token_id=tokenizer.eos_token_id,\n",
                "        pad_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "\n",
                "print(f\"Output tokens: {outputs.shape}\")\n",
                "print(f\"New tokens generated: {outputs.shape[1] - inputs['input_ids'].shape[1]}\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# RAW output (no processing)\n",
                "raw_output = tokenizer.decode(outputs[0])\n",
                "print(\"=\"*60)\n",
                "print(\"RAW OUTPUT (with special tokens):\")\n",
                "print(\"=\"*60)\n",
                "print(raw_output)\n",
                "print(\"=\"*60)"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Decode ONLY the new tokens\n",
                "input_length = inputs['input_ids'].shape[1]\n",
                "new_tokens = outputs[0][input_length:]\n",
                "new_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
                "\n",
                "print(\"=\"*60)\n",
                "print(\"ONLY NEW GENERATED TOKENS:\")\n",
                "print(\"=\"*60)\n",
                "print(f\"'{new_text}'\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "if len(new_text.strip()) == 0:\n",
                "    print(\"\\n‚ö†Ô∏è MODEL GENERATED NOTHING NEW!\")\n",
                "    print(\"This means the model is hitting EOS immediately.\")\n",
                "    print(\"The adapter may not have trained properly for generation.\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Test with sampling enabled\n",
                "print(\"\\nüß™ Testing with do_sample=True, temperature=0.7:\")\n",
                "with torch.no_grad():\n",
                "    outputs2 = model.generate(\n",
                "        **inputs,\n",
                "        max_new_tokens=50,\n",
                "        do_sample=True,\n",
                "        temperature=0.7,\n",
                "        top_p=0.9,\n",
                "        eos_token_id=tokenizer.eos_token_id\n",
                "    )\n",
                "\n",
                "new_tokens2 = outputs2[0][input_length:]\n",
                "new_text2 = tokenizer.decode(new_tokens2, skip_special_tokens=True)\n",
                "print(f\"Generated: '{new_text2}'\")"
            ],
            "metadata": {},
            "execution_count": null,
            "outputs": []
        }
    ]
}