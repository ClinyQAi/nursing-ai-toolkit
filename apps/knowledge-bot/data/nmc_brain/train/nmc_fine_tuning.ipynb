{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Nursing Proficiency+ Model Training\n",
                "\n",
                "This notebook fine-tunes the MedGemma 4B model on the custom NMC dataset. \n",
                "It supports persistent checkpointing to Google Drive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mount Google Drive\n",
                "This ensures checkpoints are saved if the runtime disconnects."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import os\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Create checkpoint directory\n",
                "checkpoint_dir = \"/content/drive/My Drive/nursing-proficiency-plus-checkpoints\"\n",
                "os.makedirs(checkpoint_dir, exist_ok=True)\n",
                "print(f\"Checkpoints will be saved to: {checkpoint_dir}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q -U torch bitsandbytes git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/trl.git accelerate datasets huggingface_hub"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Hugging Face Login"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import login\n",
                "# Paste your token below when prompted or hardcode it\n",
                "login()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Run Training\n",
                "Make sure you have uploaded `nmc_dataset_web.jsonl` to the `data/` folder in Colab files.\n",
                "\n",
                "**Note:** You might need to create the `data` folder first: `!mkdir -p data` and upload the file there."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import torch\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    BitsAndBytesConfig,\n",
                "    TrainingArguments\n",
                ")\n",
                "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "\n",
                "# --- Configuration ---\n",
                "model_id = \"google/medgemma-4b-it\"\n",
                "# Updated Path based on User input\n",
                "dataset_path = \"/content/drive/MyDrive/nmc_brain/data/nmc_dataset_web.jsonl\"\n",
                "output_dir = \"/content/drive/My Drive/nursing-proficiency-plus-checkpoints\"\n",
                "hub_model_id = \"NurseCitizenDeveloper/nursing-proficiency-plus\"\n",
                "\n",
                "# Verify dataset exists\n",
                "if not os.path.exists(dataset_path):\n",
                "    print(f\"ERROR: Dataset not found at {dataset_path}. Please check your Drive folders!\")\n",
                "else:\n",
                "    print(\"Dataset found. Starting setup...\")\n",
                "\n",
                "    # 1. Load Dataset\n",
                "    dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
                "    print(f\"Loaded {len(dataset)} examples\")\n",
                "\n",
                "    # 2. BitsAndBytes Config\n",
                "    bnb_config = BitsAndBytesConfig(\n",
                "        load_in_4bit=True,\n",
                "        bnb_4bit_quant_type=\"nf4\",\n",
                "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "        bnb_4bit_use_double_quant=True\n",
                "    )\n",
                "\n",
                "    # 3. Load Model\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        model_id,\n",
                "        quantization_config=bnb_config,\n",
                "        device_map=\"auto\",\n",
                "        trust_remote_code=True\n",
                "    )\n",
                "    model.config.use_cache = False\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
                "    tokenizer.pad_token = tokenizer.eos_token\n",
                "    # Fix for fp16\n",
                "    tokenizer.padding_side = \"right\"\n",
                "\n",
                "    # 4. LoRA Config\n",
                "    peft_config = LoraConfig(\n",
                "        lora_alpha=16,\n",
                "        lora_dropout=0.1,\n",
                "        r=64,\n",
                "        bias=\"none\",\n",
                "        task_type=\"CAUSAL_LM\",\n",
                "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
                "    )\n",
                "\n",
                "    # 5. Training Args\n",
                "    sft_config = SFTConfig(output_dir=output_dir)\n",
                "    # sft_config.dataset_text_field = \"output\" # COMMENTED OUT to use formatting_func\n",
                "    sft_config.max_seq_length = 1024\n",
                "    sft_config.num_train_epochs = 3\n",
                "    sft_config.per_device_train_batch_size = 4\n",
                "    sft_config.gradient_accumulation_steps = 4\n",
                "    sft_config.optim = \"paged_adamw_32bit\"\n",
                "    sft_config.save_strategy = \"steps\"\n",
                "    sft_config.save_steps = 50\n",
                "    sft_config.save_total_limit = 2\n",
                "    sft_config.logging_steps = 10\n",
                "    sft_config.learning_rate = 2e-4\n",
                "    sft_config.weight_decay = 0.001\n",
                "    sft_config.bf16 = True\n",
                "    sft_config.push_to_hub = True\n",
                "    sft_config.hub_model_id = hub_model_id\n",
                "    sft_config.hub_private_repo = True\n",
                "    sft_config.hub_strategy = \"checkpoint\"\n",
                "    sft_config.report_to = \"none\"\n",
                "    sft_config.packing = False \n",
                "\n",
                "    # --- 1. Formatting Function (Combine Instruction + Output) ---\n",
                "    def formatting_prompts_func(example):\n",
                "        output_texts = []\n",
                "        # Robust check for single string vs list\n",
                "        instructions = example['instruction']\n",
                "        outputs = example['output']\n",
                "        \n",
                "        if isinstance(instructions, str):\n",
                "            instructions = [instructions]\n",
                "            outputs = [outputs]\n",
                "            \n",
                "        for i in range(len(instructions)):\n",
                "            inst = instructions[i]\n",
                "            out = outputs[i] if i < len(outputs) else \"error\"\n",
                "            \n",
                "            # Standard Alpaca-style or simple User/Assistant format\n",
                "            text = f\"<start_of_turn>user\\n{inst}<end_of_turn>\\n<start_of_turn>model\\n{out}<end_of_turn>\"\n",
                "            output_texts.append(text)\n",
                "        \n",
                "        return output_texts\n",
                "\n",
                "    # PRE-PROCESSING: Apply formatting manually to avoid 'add_eos' TRL bugs\n",
                "    print(\"Pre-formatting dataset...\")\n",
                "    # Map the formatting function to create a 'text' column\n",
                "    dataset = dataset.map(lambda x: {\"text\": formatting_prompts_func(x)}, batched=True)\n",
                "\n",
                "    # --- 2. Custom Collator (Fixes Gemma 3 & Labels) ---\n",
                "    def data_collator(features):\n",
                "        batch = tokenizer.pad(features, padding=True, return_tensors=\"pt\")\n",
                "        batch = dict(batch) \n",
                "        \n",
                "        if \"token_type_ids\" not in batch:\n",
                "            batch[\"token_type_ids\"] = torch.zeros_like(batch[\"input_ids\"])\n",
                "            \n",
                "        if \"labels\" not in batch:\n",
                "            labels = batch[\"input_ids\"].clone()\n",
                "            if tokenizer.pad_token_id is not None:\n",
                "                labels[labels == tokenizer.pad_token_id] = -100\n",
                "            batch[\"labels\"] = labels\n",
                "            \n",
                "        return batch\n",
                "\n",
                "    sft_config.dataset_text_field = \"text\" # Point to our new column\n",
                "\n",
                "    # 6. Trainer\n",
                "    trainer = SFTTrainer(\n",
                "        model=model, \n",
                "        train_dataset=dataset, \n",
                "        peft_config=peft_config, \n",
                "        processing_class=tokenizer,\n",
                "        args=sft_config,\n",
                "        # formatting_func=formatting_prompts_func, # REMOVED\n",
                "        data_collator=data_collator \n",
                "    )\n",
                "\n",
                "    # 7. Train (Resume Check)\n",
                "    checkpoint = None\n",
                "    if os.path.isdir(output_dir) and any(d.startswith(\"checkpoint\") for d in os.listdir(output_dir)):\n",
                "        checkpoint = True\n",
                "        print(f\"Resuming from checkpoint in {output_dir}\")\n",
                "    \n",
                "    trainer.train(resume_from_checkpoint=checkpoint)\n",
                "    \n",
                "    # 8. Save & Push\n",
                "    trainer.model.save_pretrained(output_dir)\n",
                "    tokenizer.save_pretrained(output_dir)\n",
                "    trainer.push_to_hub()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}